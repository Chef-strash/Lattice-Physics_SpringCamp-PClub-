{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# In this part i have performed Hyperparametric tuning using Optuna.\n",
        "\n",
        "I have copied certian part of my code from other notebook and performed Tuning on it."
      ],
      "metadata": {
        "id": "PEppv8qTtZ3w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5844JV_-KCFy"
      },
      "outputs": [],
      "source": [
        "#Libraries for Numeric analysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "\n",
        "#Libraries for Visual representation of data\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "#Library to remove uncecessary waarnings messages.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('raw.csv', header=None)\n",
        "print(df.shape)\n",
        "\n",
        "#Convert the single row of space-separated values into multiple columns\n",
        "df=pd.DataFrame(df[0].str.split(' ').tolist())\n",
        "\n",
        "#Rename columns\n",
        "column_names=['KINF', 'PPPF'] + [f'C{i}' for i in range(1, df.shape[1] - 1)]\n",
        "df.columns=column_names[:df.shape[1]]\n",
        "\n",
        "\n",
        "#Save the resulting DataFrame to a new CSV file named \"processed.csv\"\n",
        "df.to_csv('processed.csv', index=False)\n",
        "df_1=pd.read_csv('processed.csv')\n",
        "print(df_1.shape)\n",
        "\n",
        "df_1.drop(\"C40\", axis=1, inplace=True)\n",
        "df.to_csv('processed.csv', index=False)\n",
        "print(df_1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2iBxktxN0Yy",
        "outputId": "33d0d4d9-6efc-4eee-cade-c4b566b40681"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24000, 1)\n",
            "(24000, 42)\n",
            "(24000, 41)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TC9jfT6GNe7x"
      },
      "outputs": [],
      "source": [
        "#Importing necessary Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "#These LIbraries are used for creating the CNN architecture.\n",
        "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout, Input, LeakyReLU, BatchNormalization\n",
        "\n",
        "#This Library is used to Normalize the the dataset which is part of pre-processing.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=df_1.drop(columns=['PPPF'])\n",
        "Z=df_1['PPPF']\n",
        "\n",
        "#Agian This code snippet aims to select the top performing featues.\n",
        "#In this case after trying many value for \"num_features\" 39 came out to be the best(It didnt really help).\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "num_features=39\n",
        "selector=SelectKBest(score_func=f_regression, k=num_features)\n",
        "X_selected=selector.fit_transform(X, Z)\n",
        "\n",
        "X_train,X_test,Z_train,Z_test,=tts(X_selected,Z,test_size=0.3,random_state=64)"
      ],
      "metadata": {
        "id": "QUYczvn_NhhM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler=StandardScaler()\n",
        "X_train=scaler.fit_transform(X_train)\n",
        "X_test=scaler.transform(X_test)\n",
        "\n",
        "# Reshape input for CNN (add a channel dimension for Conv1D).A Conv1D layer expects input data to have a 3-dimensional shape\n",
        "X_train_cnn=np.expand_dims(X_train, axis=-1)\n",
        "X_test_cnn=np.expand_dims(X_test, axis=-1)\n",
        "print(X_train_cnn.shape)\n",
        "print(X_test_cnn.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbe9BwLwNkEp",
        "outputId": "ceca77bf-a02d-4603-825d-81649153e450"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16800, 39, 1)\n",
            "(7200, 39, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This was a trial run\n",
        "\n",
        "This was the models performance Before Tuning."
      ],
      "metadata": {
        "id": "_qLZEPzZt878"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#L2 regularization is a technique used to prevent overfitting in machine learning models.\n",
        "#Id does so by adding a penalty to the loss function based on the squared magnitude of the model's weights.(Learning from mistakes)\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "#CNN model's Architecture.\n",
        "model = Sequential([\n",
        "        Input(shape=(X_train_cnn.shape[1], X_train_cnn.shape[2])),\n",
        "        Conv1D(128, kernel_size=3,activation='relu'),\n",
        "        Dropout((0.2)),\n",
        "        Conv1D(64, kernel_size=3,activation='relu'),#pehale 64 tha\n",
        "        Dropout((0.2)),\n",
        "        Flatten(),\n",
        "        Dense(64, kernel_regularizer=l2(0.01),activation='relu'),\n",
        "        Dropout((0.2)),\n",
        "        Dense(32,activation='relu'),\n",
        "        Dense(1)\n",
        "])\n",
        "\n",
        "#Compiling the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "#Adding callbacks for training\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
        "\n",
        "#Training the model\n",
        "history = model.fit(\n",
        "    X_train_cnn, Z_train,\n",
        "    validation_data=(X_test_cnn, Z_test),\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ")\n",
        "\n",
        "#Evaluating the model based on Mean Absolute error.\n",
        "loss, mae = model.evaluate(X_test_cnn, Z_test, verbose=1)\n",
        "print(f\"Test Mean Absolute Error: {mae}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo_ZhnBwNmoq",
        "outputId": "790b267e-4c99-4e22-e4d1-b52e5e71b3aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - loss: 0.6097 - mae: 0.2909 - val_loss: 0.0924 - val_mae: 0.2723 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - loss: 0.0271 - mae: 0.1064 - val_loss: 0.0951 - val_mae: 0.2924 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - loss: 0.0151 - mae: 0.0834 - val_loss: 0.0626 - val_mae: 0.2341 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 0.0109 - mae: 0.0725 - val_loss: 0.0287 - val_mae: 0.1472 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - loss: 0.0084 - mae: 0.0652 - val_loss: 0.0163 - val_mae: 0.1047 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - loss: 0.0080 - mae: 0.0644 - val_loss: 0.0244 - val_mae: 0.1320 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 0.0077 - mae: 0.0632 - val_loss: 0.0142 - val_mae: 0.0952 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - loss: 0.0076 - mae: 0.0634 - val_loss: 0.0150 - val_mae: 0.0986 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0074 - mae: 0.0625 - val_loss: 0.0118 - val_mae: 0.0844 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 21ms/step - loss: 0.0075 - mae: 0.0625 - val_loss: 0.0115 - val_mae: 0.0833 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - loss: 0.0070 - mae: 0.0597 - val_loss: 0.0194 - val_mae: 0.1162 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - loss: 0.0070 - mae: 0.0592 - val_loss: 0.0139 - val_mae: 0.0946 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.0065 - mae: 0.0568 - val_loss: 0.0208 - val_mae: 0.1209 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0062 - mae: 0.0554 - val_loss: 0.0097 - val_mae: 0.0739 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0062 - mae: 0.0559 - val_loss: 0.0090 - val_mae: 0.0703 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - loss: 0.0058 - mae: 0.0537 - val_loss: 0.0082 - val_mae: 0.0674 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - loss: 0.0053 - mae: 0.0516 - val_loss: 0.0086 - val_mae: 0.0706 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - loss: 0.0055 - mae: 0.0521 - val_loss: 0.0128 - val_mae: 0.0929 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0053 - mae: 0.0517 - val_loss: 0.0122 - val_mae: 0.0902 - learning_rate: 0.0010\n",
            "Epoch 20/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 23ms/step - loss: 0.0053 - mae: 0.0513 - val_loss: 0.0150 - val_mae: 0.1023 - learning_rate: 0.0010\n",
            "Epoch 21/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0051 - mae: 0.0507 - val_loss: 0.0114 - val_mae: 0.0869 - learning_rate: 0.0010\n",
            "Epoch 22/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 0.0045 - mae: 0.0477 - val_loss: 0.0109 - val_mae: 0.0862 - learning_rate: 5.0000e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0046 - mae: 0.0491 - val_loss: 0.0121 - val_mae: 0.0921 - learning_rate: 5.0000e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0044 - mae: 0.0478 - val_loss: 0.0078 - val_mae: 0.0694 - learning_rate: 5.0000e-04\n",
            "Epoch 25/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - loss: 0.0044 - mae: 0.0480 - val_loss: 0.0166 - val_mae: 0.1121 - learning_rate: 5.0000e-04\n",
            "Epoch 26/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0046 - mae: 0.0488 - val_loss: 0.0150 - val_mae: 0.1049 - learning_rate: 5.0000e-04\n",
            "Epoch 27/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0044 - mae: 0.0479 - val_loss: 0.0106 - val_mae: 0.0853 - learning_rate: 5.0000e-04\n",
            "Epoch 28/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 0.0044 - mae: 0.0484 - val_loss: 0.0100 - val_mae: 0.0822 - learning_rate: 5.0000e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0044 - mae: 0.0475 - val_loss: 0.0116 - val_mae: 0.0901 - learning_rate: 5.0000e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - loss: 0.0042 - mae: 0.0469 - val_loss: 0.0125 - val_mae: 0.0953 - learning_rate: 2.5000e-04\n",
            "Epoch 31/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0040 - mae: 0.0465 - val_loss: 0.0104 - val_mae: 0.0856 - learning_rate: 2.5000e-04\n",
            "Epoch 32/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 20ms/step - loss: 0.0040 - mae: 0.0464 - val_loss: 0.0098 - val_mae: 0.0825 - learning_rate: 2.5000e-04\n",
            "Epoch 33/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - loss: 0.0039 - mae: 0.0459 - val_loss: 0.0129 - val_mae: 0.0971 - learning_rate: 2.5000e-04\n",
            "Epoch 34/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0039 - mae: 0.0461 - val_loss: 0.0099 - val_mae: 0.0823 - learning_rate: 2.5000e-04\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0078 - mae: 0.0694\n",
            "Test Mean Absolute Error: 0.06944114714860916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparametric tuning using Optuna"
      ],
      "metadata": {
        "id": "RPjFCOvKuIyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkVbQNaubzvv",
        "outputId": "574bd2f7-c172-480c-f89a-6e0f821201f7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.37)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.2.0-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.4/383.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.8 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*AI use in implementing Optuna Hyperparametric Tuning*"
      ],
      "metadata": {
        "id": "T_v-C0na0kNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameters to optimize\n",
        "    num_filters_1 = trial.suggest_int('num_filters', 32, 200, step=32)\n",
        "    num_filters_2 = trial.suggest_int('num_filters', 32, 200, step=32)\n",
        "    num_filters_3 = trial.suggest_int('num_filters', 32, 200, step=32)\n",
        "    kernel_size_1 = trial.suggest_int('kernel_size', 3, 7)\n",
        "    kernel_size_2 = trial.suggest_int('kernel_size', 3, 7)\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
        "    dense_units = trial.suggest_int('dense_units', 64, 256, step=64)\n",
        "\n",
        "    model = Sequential([\n",
        "        Input(shape=(X_train_cnn.shape[1], X_train_cnn.shape[2])),\n",
        "        Conv1D(num_filters_1, kernel_size=kernel_size_1),\n",
        "        LeakyReLU(),\n",
        "        Conv1D(num_filters_2, kernel_size=kernel_size_2),\n",
        "        LeakyReLU(),\n",
        "        Flatten(),\n",
        "        Dense(dense_units, kernel_regularizer=regularizers.l2(0.01)),\n",
        "        LeakyReLU(),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(num_filters_3),\n",
        "        LeakyReLU(),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "    history = model.fit(X_train_cnn, Z_train, validation_data=(X_test_cnn, Z_test),\n",
        "                        epochs=10, batch_size=64, verbose=0)\n",
        "\n",
        "    loss, mae = model.evaluate(X_test_cnn, Z_test, verbose=0)\n",
        "    return mae  # Return the evaluation metric you want to optimize\n",
        "\n",
        "# Create study and optimize\n",
        "study = optuna.create_study(direction='minimize')  # Minimize MAE\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(f\"Value: {trial.value}\")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"  {key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7KWOXnTbvcj",
        "outputId": "75a24b5c-c1f3-4632-d9a8-9fffba0aa381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-26 11:48:20,388] A new study created in memory with name: no-name-d77c82f2-2e35-4271-a6c7-21c3c7cf0497\n",
            "[I 2025-01-26 11:55:39,575] Trial 0 finished with value: 0.056845489889383316 and parameters: {'num_filters': 192, 'kernel_size': 6, 'dropout_rate': 0.3412511414349714, 'dense_units': 256}. Best is trial 0 with value: 0.056845489889383316.\n",
            "[I 2025-01-26 11:56:52,570] Trial 1 finished with value: 0.05574087053537369 and parameters: {'num_filters': 64, 'kernel_size': 4, 'dropout_rate': 0.4707488435477307, 'dense_units': 256}. Best is trial 1 with value: 0.05574087053537369.\n",
            "[I 2025-01-26 11:57:40,490] Trial 2 finished with value: 0.07907459884881973 and parameters: {'num_filters': 64, 'kernel_size': 4, 'dropout_rate': 0.2854277940777422, 'dense_units': 64}. Best is trial 1 with value: 0.05574087053537369.\n",
            "[I 2025-01-26 11:58:30,074] Trial 3 finished with value: 0.057668041437864304 and parameters: {'num_filters': 64, 'kernel_size': 6, 'dropout_rate': 0.22481167554009304, 'dense_units': 64}. Best is trial 1 with value: 0.05574087053537369.\n",
            "[I 2025-01-26 12:01:09,776] Trial 4 finished with value: 0.06722351163625717 and parameters: {'num_filters': 160, 'kernel_size': 3, 'dropout_rate': 0.2059926371747917, 'dense_units': 64}. Best is trial 1 with value: 0.05574087053537369.\n",
            "[I 2025-01-26 12:01:50,376] Trial 5 finished with value: 0.0598599873483181 and parameters: {'num_filters': 32, 'kernel_size': 5, 'dropout_rate': 0.253311896399444, 'dense_units': 192}. Best is trial 1 with value: 0.05574087053537369.\n",
            "[I 2025-01-26 12:02:23,323] Trial 6 finished with value: 0.05728556588292122 and parameters: {'num_filters': 32, 'kernel_size': 4, 'dropout_rate': 0.226456603235906, 'dense_units': 192}. Best is trial 1 with value: 0.05574087053537369.\n",
            "[I 2025-01-26 12:05:27,063] Trial 7 finished with value: 0.05602245032787323 and parameters: {'num_filters': 128, 'kernel_size': 5, 'dropout_rate': 0.23895491604195837, 'dense_units': 192}. Best is trial 1 with value: 0.05574087053537369.\n",
            "[I 2025-01-26 12:10:16,935] Trial 8 finished with value: 0.05613558366894722 and parameters: {'num_filters': 160, 'kernel_size': 7, 'dropout_rate': 0.2300859162692671, 'dense_units': 192}. Best is trial 1 with value: 0.05574087053537369.\n",
            "[I 2025-01-26 12:12:36,515] Trial 9 finished with value: 0.05541137605905533 and parameters: {'num_filters': 128, 'kernel_size': 3, 'dropout_rate': 0.24231081943787744, 'dense_units': 192}. Best is trial 9 with value: 0.05541137605905533.\n",
            "[I 2025-01-26 12:14:15,680] Trial 10 finished with value: 0.055788811296224594 and parameters: {'num_filters': 128, 'kernel_size': 3, 'dropout_rate': 0.44002801504339073, 'dense_units': 128}. Best is trial 9 with value: 0.05541137605905533.\n",
            "[I 2025-01-26 12:15:56,481] Trial 11 finished with value: 0.05792747065424919 and parameters: {'num_filters': 96, 'kernel_size': 4, 'dropout_rate': 0.49712865719996524, 'dense_units': 256}. Best is trial 9 with value: 0.05541137605905533.\n",
            "[I 2025-01-26 12:17:40,837] Trial 12 finished with value: 0.058977577835321426 and parameters: {'num_filters': 96, 'kernel_size': 3, 'dropout_rate': 0.39699710771063657, 'dense_units': 256}. Best is trial 9 with value: 0.05541137605905533.\n",
            "[I 2025-01-26 12:18:43,632] Trial 13 finished with value: 0.05681917816400528 and parameters: {'num_filters': 64, 'kernel_size': 4, 'dropout_rate': 0.3262939537153807, 'dense_units': 128}. Best is trial 9 with value: 0.05541137605905533.\n",
            "[I 2025-01-26 12:20:35,743] Trial 14 finished with value: 0.055315159261226654 and parameters: {'num_filters': 96, 'kernel_size': 3, 'dropout_rate': 0.4061945073582955, 'dense_units': 256}. Best is trial 14 with value: 0.055315159261226654.\n",
            "[I 2025-01-26 12:22:16,642] Trial 15 finished with value: 0.05499938130378723 and parameters: {'num_filters': 128, 'kernel_size': 3, 'dropout_rate': 0.3950859410642938, 'dense_units': 128}. Best is trial 15 with value: 0.05499938130378723.\n",
            "[I 2025-01-26 12:25:37,177] Trial 16 finished with value: 0.056138262152671814 and parameters: {'num_filters': 160, 'kernel_size': 3, 'dropout_rate': 0.3958512402262292, 'dense_units': 128}. Best is trial 15 with value: 0.05499938130378723.\n",
            "[I 2025-01-26 12:27:15,799] Trial 17 finished with value: 0.06114364042878151 and parameters: {'num_filters': 96, 'kernel_size': 5, 'dropout_rate': 0.3971264466058189, 'dense_units': 128}. Best is trial 15 with value: 0.05499938130378723.\n",
            "[I 2025-01-26 12:33:07,379] Trial 18 finished with value: 0.05299017205834389 and parameters: {'num_filters': 192, 'kernel_size': 6, 'dropout_rate': 0.3646740530968014, 'dense_units': 128}. Best is trial 18 with value: 0.05299017205834389.\n",
            "[I 2025-01-26 12:39:34,661] Trial 19 finished with value: 0.05423372611403465 and parameters: {'num_filters': 192, 'kernel_size': 7, 'dropout_rate': 0.3078669359237609, 'dense_units': 128}. Best is trial 18 with value: 0.05299017205834389.\n",
            "[I 2025-01-26 12:46:13,375] Trial 20 finished with value: 0.09443000704050064 and parameters: {'num_filters': 192, 'kernel_size': 7, 'dropout_rate': 0.3069108239614709, 'dense_units': 64}. Best is trial 18 with value: 0.05299017205834389.\n",
            "[I 2025-01-26 12:52:02,586] Trial 21 finished with value: 0.05255721136927605 and parameters: {'num_filters': 192, 'kernel_size': 6, 'dropout_rate': 0.3642835907540115, 'dense_units': 128}. Best is trial 21 with value: 0.05255721136927605.\n",
            "[I 2025-01-26 12:58:10,038] Trial 22 finished with value: 0.056481506675481796 and parameters: {'num_filters': 192, 'kernel_size': 6, 'dropout_rate': 0.34695506064897647, 'dense_units': 128}. Best is trial 21 with value: 0.05255721136927605.\n",
            "[I 2025-01-26 13:04:53,387] Trial 23 finished with value: 0.059101738035678864 and parameters: {'num_filters': 192, 'kernel_size': 7, 'dropout_rate': 0.3618148424642759, 'dense_units': 128}. Best is trial 21 with value: 0.05255721136927605.\n",
            "[I 2025-01-26 13:09:48,199] Trial 24 finished with value: 0.05603863671422005 and parameters: {'num_filters': 160, 'kernel_size': 6, 'dropout_rate': 0.2833405207756767, 'dense_units': 128}. Best is trial 21 with value: 0.05255721136927605.\n",
            "[I 2025-01-26 13:15:40,518] Trial 25 finished with value: 0.11610229313373566 and parameters: {'num_filters': 192, 'kernel_size': 7, 'dropout_rate': 0.3638376005348783, 'dense_units': 64}. Best is trial 21 with value: 0.05255721136927605.\n",
            "[I 2025-01-26 13:19:04,237] Trial 26 finished with value: 0.055810365825891495 and parameters: {'num_filters': 160, 'kernel_size': 6, 'dropout_rate': 0.3181831122347959, 'dense_units': 128}. Best is trial 21 with value: 0.05255721136927605.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I stopped further searching for better parameters as for many previous iterations the performance wasnt increasing."
      ],
      "metadata": {
        "id": "zfBeqESMxcON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model's performance after Tuning the model and manual tweaking."
      ],
      "metadata": {
        "id": "sDG_8DbMuSLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#L2 regularization is a technique used to prevent overfitting in machine learning models.\n",
        "#Id does so by adding a penalty to the loss function based on the squared magnitude of the model's weights.(Learning from mistakes)\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "#CNN model's Architecture.\n",
        "model = Sequential([\n",
        "        Input(shape=(X_train_cnn.shape[1], X_train_cnn.shape[2])),\n",
        "        Conv1D(128, kernel_size=3,activation='relu'),\n",
        "        Dropout((0.2)),\n",
        "        Conv1D(64, kernel_size=3,activation='relu'),#pehale 64 tha\n",
        "        Dropout((0.2)),\n",
        "        Flatten(),\n",
        "        Dense(64, kernel_regularizer=l2(0.1),activation='relu'),\n",
        "        Dropout((0.2)),\n",
        "        Dense(32,activation='relu'),\n",
        "        Dense(1)\n",
        "])\n",
        "\n",
        "#Compiling the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "#Adding callbacks for training\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
        "\n",
        "#Training the model\n",
        "history = model.fit(\n",
        "    X_train_cnn, Z_train,\n",
        "    validation_data=(X_test_cnn, Z_test),\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ")\n",
        "\n",
        "#Evaluating the model based on Mean Absolute error.\n",
        "loss, mae = model.evaluate(X_test_cnn, Z_test, verbose=1)\n",
        "print(f\"Test Mean Absolute Error: {mae}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ8yJWVVT5Hw",
        "outputId": "c6c86d71-c7a8-4233-8663-268afe1f8bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - loss: 2.1233 - mae: 0.2618 - val_loss: 0.0146 - val_mae: 0.0682 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0281 - mae: 0.1184 - val_loss: 0.0092 - val_mae: 0.0598 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0151 - mae: 0.0891 - val_loss: 0.0079 - val_mae: 0.0614 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0117 - mae: 0.0781 - val_loss: 0.0102 - val_mae: 0.0698 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0100 - mae: 0.0707 - val_loss: 0.0079 - val_mae: 0.0614 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - loss: 0.0094 - mae: 0.0680 - val_loss: 0.0091 - val_mae: 0.0681 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0092 - mae: 0.0673 - val_loss: 0.0077 - val_mae: 0.0611 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - loss: 0.0090 - mae: 0.0671 - val_loss: 0.0076 - val_mae: 0.0609 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - loss: 0.0090 - mae: 0.0669 - val_loss: 0.0077 - val_mae: 0.0612 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0090 - mae: 0.0668 - val_loss: 0.0085 - val_mae: 0.0646 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0088 - mae: 0.0660 - val_loss: 0.0073 - val_mae: 0.0572 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0086 - mae: 0.0647 - val_loss: 0.0081 - val_mae: 0.0609 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 0.0089 - mae: 0.0652 - val_loss: 0.0093 - val_mae: 0.0667 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - loss: 0.0085 - mae: 0.0640 - val_loss: 0.0071 - val_mae: 0.0550 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - loss: 0.0085 - mae: 0.0638 - val_loss: 0.0078 - val_mae: 0.0585 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 20ms/step - loss: 0.0084 - mae: 0.0625 - val_loss: 0.0068 - val_mae: 0.0550 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - loss: 0.0078 - mae: 0.0610 - val_loss: 0.0077 - val_mae: 0.0590 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - loss: 0.0079 - mae: 0.0606 - val_loss: 0.0073 - val_mae: 0.0586 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0078 - mae: 0.0603 - val_loss: 0.0069 - val_mae: 0.0543 - learning_rate: 0.0010\n",
            "Epoch 20/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0073 - mae: 0.0586 - val_loss: 0.0066 - val_mae: 0.0539 - learning_rate: 0.0010\n",
            "Epoch 21/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0075 - mae: 0.0592 - val_loss: 0.0059 - val_mae: 0.0520 - learning_rate: 0.0010\n",
            "Epoch 22/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0073 - mae: 0.0589 - val_loss: 0.0069 - val_mae: 0.0534 - learning_rate: 0.0010\n",
            "Epoch 23/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - loss: 0.0072 - mae: 0.0580 - val_loss: 0.0066 - val_mae: 0.0524 - learning_rate: 0.0010\n",
            "Epoch 24/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - loss: 0.0073 - mae: 0.0584 - val_loss: 0.0061 - val_mae: 0.0508 - learning_rate: 0.0010\n",
            "Epoch 25/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 22ms/step - loss: 0.0071 - mae: 0.0580 - val_loss: 0.0057 - val_mae: 0.0502 - learning_rate: 0.0010\n",
            "Epoch 26/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - loss: 0.0072 - mae: 0.0582 - val_loss: 0.0075 - val_mae: 0.0595 - learning_rate: 0.0010\n",
            "Epoch 27/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - loss: 0.0075 - mae: 0.0597 - val_loss: 0.0058 - val_mae: 0.0514 - learning_rate: 0.0010\n",
            "Epoch 28/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0070 - mae: 0.0577 - val_loss: 0.0063 - val_mae: 0.0535 - learning_rate: 0.0010\n",
            "Epoch 29/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 0.0070 - mae: 0.0574 - val_loss: 0.0059 - val_mae: 0.0516 - learning_rate: 0.0010\n",
            "Epoch 30/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0072 - mae: 0.0585 - val_loss: 0.0059 - val_mae: 0.0511 - learning_rate: 0.0010\n",
            "Epoch 31/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 0.0062 - mae: 0.0562 - val_loss: 0.0051 - val_mae: 0.0491 - learning_rate: 5.0000e-04\n",
            "Epoch 32/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - loss: 0.0061 - mae: 0.0555 - val_loss: 0.0054 - val_mae: 0.0503 - learning_rate: 5.0000e-04\n",
            "Epoch 33/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0062 - mae: 0.0557 - val_loss: 0.0052 - val_mae: 0.0499 - learning_rate: 5.0000e-04\n",
            "Epoch 34/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0060 - mae: 0.0557 - val_loss: 0.0050 - val_mae: 0.0497 - learning_rate: 5.0000e-04\n",
            "Epoch 35/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0061 - mae: 0.0556 - val_loss: 0.0049 - val_mae: 0.0482 - learning_rate: 5.0000e-04\n",
            "Epoch 36/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0062 - mae: 0.0561 - val_loss: 0.0049 - val_mae: 0.0476 - learning_rate: 5.0000e-04\n",
            "Epoch 37/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 23ms/step - loss: 0.0060 - mae: 0.0551 - val_loss: 0.0052 - val_mae: 0.0489 - learning_rate: 5.0000e-04\n",
            "Epoch 38/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0061 - mae: 0.0550 - val_loss: 0.0048 - val_mae: 0.0472 - learning_rate: 5.0000e-04\n",
            "Epoch 39/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - loss: 0.0060 - mae: 0.0547 - val_loss: 0.0050 - val_mae: 0.0479 - learning_rate: 5.0000e-04\n",
            "Epoch 40/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - loss: 0.0061 - mae: 0.0550 - val_loss: 0.0050 - val_mae: 0.0485 - learning_rate: 5.0000e-04\n",
            "Epoch 41/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - loss: 0.0060 - mae: 0.0552 - val_loss: 0.0051 - val_mae: 0.0485 - learning_rate: 5.0000e-04\n",
            "Epoch 42/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0059 - mae: 0.0541 - val_loss: 0.0050 - val_mae: 0.0486 - learning_rate: 5.0000e-04\n",
            "Epoch 43/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 0.0061 - mae: 0.0551 - val_loss: 0.0059 - val_mae: 0.0531 - learning_rate: 5.0000e-04\n",
            "Epoch 44/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0054 - mae: 0.0521 - val_loss: 0.0045 - val_mae: 0.0471 - learning_rate: 2.5000e-04\n",
            "Epoch 45/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0056 - mae: 0.0541 - val_loss: 0.0044 - val_mae: 0.0469 - learning_rate: 2.5000e-04\n",
            "Epoch 46/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 0.0055 - mae: 0.0535 - val_loss: 0.0045 - val_mae: 0.0474 - learning_rate: 2.5000e-04\n",
            "Epoch 47/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0053 - mae: 0.0530 - val_loss: 0.0044 - val_mae: 0.0473 - learning_rate: 2.5000e-04\n",
            "Epoch 48/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 22ms/step - loss: 0.0054 - mae: 0.0535 - val_loss: 0.0045 - val_mae: 0.0476 - learning_rate: 2.5000e-04\n",
            "Epoch 49/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 0.0052 - mae: 0.0522 - val_loss: 0.0052 - val_mae: 0.0526 - learning_rate: 2.5000e-04\n",
            "Epoch 50/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0055 - mae: 0.0537 - val_loss: 0.0048 - val_mae: 0.0501 - learning_rate: 2.5000e-04\n",
            "Epoch 51/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0049 - mae: 0.0511 - val_loss: 0.0041 - val_mae: 0.0463 - learning_rate: 1.2500e-04\n",
            "Epoch 52/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0048 - mae: 0.0512 - val_loss: 0.0040 - val_mae: 0.0458 - learning_rate: 1.2500e-04\n",
            "Epoch 53/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0049 - mae: 0.0519 - val_loss: 0.0041 - val_mae: 0.0462 - learning_rate: 1.2500e-04\n",
            "Epoch 54/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0048 - mae: 0.0509 - val_loss: 0.0039 - val_mae: 0.0452 - learning_rate: 1.2500e-04\n",
            "Epoch 55/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 0.0047 - mae: 0.0506 - val_loss: 0.0040 - val_mae: 0.0454 - learning_rate: 1.2500e-04\n",
            "Epoch 56/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0048 - mae: 0.0512 - val_loss: 0.0039 - val_mae: 0.0451 - learning_rate: 1.2500e-04\n",
            "Epoch 57/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0048 - mae: 0.0515 - val_loss: 0.0042 - val_mae: 0.0469 - learning_rate: 1.2500e-04\n",
            "Epoch 58/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 0.0048 - mae: 0.0514 - val_loss: 0.0039 - val_mae: 0.0450 - learning_rate: 1.2500e-04\n",
            "Epoch 59/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 0.0047 - mae: 0.0509 - val_loss: 0.0041 - val_mae: 0.0464 - learning_rate: 1.2500e-04\n",
            "Epoch 60/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0046 - mae: 0.0501 - val_loss: 0.0037 - val_mae: 0.0448 - learning_rate: 6.2500e-05\n",
            "Epoch 61/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 23ms/step - loss: 0.0045 - mae: 0.0502 - val_loss: 0.0037 - val_mae: 0.0445 - learning_rate: 6.2500e-05\n",
            "Epoch 62/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 0.0044 - mae: 0.0499 - val_loss: 0.0037 - val_mae: 0.0444 - learning_rate: 6.2500e-05\n",
            "Epoch 63/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 0.0045 - mae: 0.0503 - val_loss: 0.0037 - val_mae: 0.0447 - learning_rate: 6.2500e-05\n",
            "Epoch 64/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0045 - mae: 0.0504 - val_loss: 0.0038 - val_mae: 0.0452 - learning_rate: 6.2500e-05\n",
            "Epoch 65/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0046 - mae: 0.0508 - val_loss: 0.0038 - val_mae: 0.0452 - learning_rate: 6.2500e-05\n",
            "Epoch 66/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.0044 - mae: 0.0495 - val_loss: 0.0036 - val_mae: 0.0444 - learning_rate: 3.1250e-05\n",
            "Epoch 67/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0043 - mae: 0.0498 - val_loss: 0.0036 - val_mae: 0.0441 - learning_rate: 3.1250e-05\n",
            "Epoch 68/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0044 - mae: 0.0496 - val_loss: 0.0036 - val_mae: 0.0448 - learning_rate: 3.1250e-05\n",
            "Epoch 69/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0044 - mae: 0.0500 - val_loss: 0.0035 - val_mae: 0.0440 - learning_rate: 3.1250e-05\n",
            "Epoch 70/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0043 - mae: 0.0494 - val_loss: 0.0035 - val_mae: 0.0441 - learning_rate: 3.1250e-05\n",
            "Epoch 71/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 22ms/step - loss: 0.0043 - mae: 0.0491 - val_loss: 0.0036 - val_mae: 0.0446 - learning_rate: 3.1250e-05\n",
            "Epoch 72/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0043 - mae: 0.0494 - val_loss: 0.0035 - val_mae: 0.0443 - learning_rate: 1.5625e-05\n",
            "Epoch 73/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0043 - mae: 0.0493 - val_loss: 0.0034 - val_mae: 0.0435 - learning_rate: 1.5625e-05\n",
            "Epoch 74/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 23ms/step - loss: 0.0042 - mae: 0.0486 - val_loss: 0.0034 - val_mae: 0.0437 - learning_rate: 1.5625e-05\n",
            "Epoch 75/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - loss: 0.0042 - mae: 0.0490 - val_loss: 0.0035 - val_mae: 0.0437 - learning_rate: 1.5625e-05\n",
            "Epoch 76/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - loss: 0.0043 - mae: 0.0494 - val_loss: 0.0035 - val_mae: 0.0441 - learning_rate: 1.5625e-05\n",
            "Epoch 77/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - loss: 0.0042 - mae: 0.0493 - val_loss: 0.0035 - val_mae: 0.0438 - learning_rate: 1.5625e-05\n",
            "Epoch 78/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0042 - mae: 0.0489 - val_loss: 0.0034 - val_mae: 0.0436 - learning_rate: 1.5625e-05\n",
            "Epoch 79/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - loss: 0.0041 - mae: 0.0490 - val_loss: 0.0034 - val_mae: 0.0434 - learning_rate: 7.8125e-06\n",
            "Epoch 80/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - loss: 0.0041 - mae: 0.0480 - val_loss: 0.0034 - val_mae: 0.0433 - learning_rate: 7.8125e-06\n",
            "Epoch 81/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - loss: 0.0043 - mae: 0.0496 - val_loss: 0.0034 - val_mae: 0.0433 - learning_rate: 7.8125e-06\n",
            "Epoch 82/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - loss: 0.0040 - mae: 0.0479 - val_loss: 0.0034 - val_mae: 0.0432 - learning_rate: 7.8125e-06\n",
            "Epoch 83/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0041 - mae: 0.0485 - val_loss: 0.0034 - val_mae: 0.0433 - learning_rate: 7.8125e-06\n",
            "Epoch 84/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 25ms/step - loss: 0.0042 - mae: 0.0491 - val_loss: 0.0034 - val_mae: 0.0433 - learning_rate: 3.9063e-06\n",
            "Epoch 85/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - loss: 0.0041 - mae: 0.0485 - val_loss: 0.0034 - val_mae: 0.0433 - learning_rate: 3.9063e-06\n",
            "Epoch 86/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - loss: 0.0040 - mae: 0.0479 - val_loss: 0.0034 - val_mae: 0.0432 - learning_rate: 3.9063e-06\n",
            "Epoch 87/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0041 - mae: 0.0484 - val_loss: 0.0034 - val_mae: 0.0431 - learning_rate: 3.9063e-06\n",
            "Epoch 88/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - loss: 0.0041 - mae: 0.0487 - val_loss: 0.0033 - val_mae: 0.0431 - learning_rate: 3.9063e-06\n",
            "Epoch 89/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - loss: 0.0040 - mae: 0.0481 - val_loss: 0.0034 - val_mae: 0.0431 - learning_rate: 1.9531e-06\n",
            "Epoch 90/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - loss: 0.0042 - mae: 0.0489 - val_loss: 0.0033 - val_mae: 0.0431 - learning_rate: 1.9531e-06\n",
            "Epoch 91/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - loss: 0.0040 - mae: 0.0477 - val_loss: 0.0033 - val_mae: 0.0431 - learning_rate: 1.9531e-06\n",
            "Epoch 92/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0040 - mae: 0.0483 - val_loss: 0.0033 - val_mae: 0.0431 - learning_rate: 1.9531e-06\n",
            "Epoch 93/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 23ms/step - loss: 0.0041 - mae: 0.0482 - val_loss: 0.0034 - val_mae: 0.0432 - learning_rate: 1.9531e-06\n",
            "Epoch 94/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - loss: 0.0041 - mae: 0.0483 - val_loss: 0.0033 - val_mae: 0.0431 - learning_rate: 9.7656e-07\n",
            "Epoch 95/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - loss: 0.0040 - mae: 0.0481 - val_loss: 0.0034 - val_mae: 0.0431 - learning_rate: 9.7656e-07\n",
            "Epoch 96/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - loss: 0.0041 - mae: 0.0485 - val_loss: 0.0033 - val_mae: 0.0431 - learning_rate: 9.7656e-07\n",
            "Epoch 97/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0041 - mae: 0.0484 - val_loss: 0.0033 - val_mae: 0.0431 - learning_rate: 9.7656e-07\n",
            "Epoch 98/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - loss: 0.0042 - mae: 0.0488 - val_loss: 0.0033 - val_mae: 0.0431 - learning_rate: 9.7656e-07\n",
            "Epoch 99/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0041 - mae: 0.0485 - val_loss: 0.0033 - val_mae: 0.0431 - learning_rate: 9.7656e-07\n",
            "Epoch 100/100\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0039 - mae: 0.0475 - val_loss: 0.0033 - val_mae: 0.0431 - learning_rate: 4.8828e-07\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0032 - mae: 0.0421\n",
            "Test Mean Absolute Error: 0.043076664209365845\n"
          ]
        }
      ]
    }
  ]
}